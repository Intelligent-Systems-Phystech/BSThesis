\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath,mathrsfs,mathtext}
\usepackage{graphicx, epsfig}
\usepackage{amsmath}
\usepackage{lineno}
\usepackage{capt-of}
\usepackage{tabu}
\usepackage{multirow}
%\usepackage{graphicx}
%\usepackage{cite}
\usepackage{indentfirst}
\usepackage{pdfpages}
\usepackage{subfig}
\usetheme{Warsaw}%{Singapore}%{Warsaw}%{Warsaw}%{Darmstadt}
\usecolortheme{sidebartab}
\definecolor{beamer@blendedblue}{RGB}{20,100,100}
%----------------------------------------------------------------------------------------------------------

\newcommand{\PP}{p}
\newcommand{\DD}{{\mathfrak{D}}}
\newcommand{\FF}{{\mathcal{F}}}
%\newcommand{\AAA}{{\mathcal{A}}}
\newcommand{\FFF}{{\mathfrak{F}}}
\newcommand{\bw}{{\textbf{w}}}
\newcommand{\ba}{{\textbf{a}}}
\newcommand{\bb}{{\textbf{b}}}
\newcommand{\II}{{\textbf{I}}}
%\newcommand{\bmu}{{\boldsymbol{\mu}}}
\newcommand{\bbf}{{\textbf{f}}}
\newcommand{\by}{{\textbf{y}}}
\newcommand{\bm}{{\textbf{m}}}
\newcommand{\bs}{{\boldsymbol{\sigma}}}
\newcommand{\al}{{\alpha}}
\newcommand{\bal}{\boldsymbol{\alpha}}
\newcommand{\bbt}{\boldsymbol{\beta}}
%\newcommand{\bA}{\mathbf{A^\text{-1}}}
\newcommand{\bAo}{\mathbf{A^\text{-1}_\text{1}}}
\newcommand{\bAt}{\mathbf{A^\text{-1}_\text{2}}}
\newcommand{\bmuo}{{\boldsymbol{\mu}_1}}
\newcommand{\bmut}{{\boldsymbol{\mu}_2}}
\newcommand{\DKL}{\mathit{D}_{\text{KL}}}

\title[\hbox to 56mm{Выбор оптимальной модели рекуррентной сети \ \  \hfill\insertframenumber\,/\,\inserttotalframenumber}]
%{Выбор оптимальной модели рекуррентной сети \\ в задачах поиска парафраза}
{Оптимизация структур сетей глубокого обучения}
%\author[А.\,Н. Смердов]{\large \\Смердов Антон Николаевич}
\author[А.\,Н. Смердов]{{\large Смердов Антон Николаевич\\~} {\small \\ Научный руководитель \\ д.ф-м.н. В.В. Стрижов}}
\institute[МФТИ]{Московский физико-технический институт \\
	Факультет управления и прикладной математики\\
	Кафедра <<Интеллектуальные системы>>
}
\date{\footnotesize{МФТИ, 13 июня 2018}}

\bibliographystyle{unsrt}

%----------------------------------------------------------------------------------------------------------
\begin{document}
%----------------------------------------------------------------------------------------------------------
\begin{frame}
	%\thispagestyle{empty}
	\titlepage
\end{frame}
%-----------------------------------------------------------------------------------------------------

\begin{frame}{Цель иследования}

\begin{block}{Проблема}
Большое количество параметров в моделях глубокого обучения влечёт сложность оптимизации параметров и переобучение.
\end{block}

\begin{block}{Цель работы}
%%Построение оптимальной модели рекуррентной нейросети в задачах поиска парафраза.
Построение модели глубокого обучения оптимальной структуры.
%Построение и последующая оптимизация структуры нейросети.
\end{block}

\begin{block}{Метод решения}
Рассматривается вариационный байесовский подход с различными предположениями о распределении вектора~параметров модели. Наименее важные параметры удаляются из сети.
\end{block}

\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Литература}
%	\nocite{*}
%	\bibliography{papers_presentation2}
	
\begin{thebibliography}{}
	\bibitem{Sanborn}
	\textit{Sanborn A., Skryzalin J.} 
	Deep Learning for Semantic Similarity // CS224d: Deep Learning for Natural Language Processing~--- Stanford, CA, USA: Stanford University, 2015. Unpublished.
	
	\bibitem{Graves}
	%	\emph{Graves A.}
	\textit{Graves A.}
	Practical variational inference for neural networks // Advances in Neural Information Processing Systems 24 (NIPS 2011). P. 2348--2356.
	
	\bibitem{StrijovBakhteev}
	\textit{О.Ю. Бахтеев, В.В. Стрижов.}
	Выбор моделей глубокого обучения cубоптимальной сложности // Автоматика и телемеханика, 2018.
\end{thebibliography}{}
	
\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Постановка задачи}
Дано $\DD = \{(\ba_i,\bb_i,y_i)\}, i = \overline{1,N}$,
$\ba_i, \bb_i$ --- последовательности векторов слов, $y_i \in \{0\dots5\}$ --- экспертная оценка их близости. 

Для модели $\bbf \in \FFF$ и вектора параметров $\bw$ определим логарифмическую функцию правдоподобия выборки:
$$L_\DD(\by,\DD, \bbf,\bw) = \log\PP(\by|\DD,\bw,\bbf) = \sum_{(\ba,\bb,y)\in\DD} \log\PP(y|\ba,\bb,\bw,\bbf)$$
Оптимальная модель $\bbf$ находится максимизацией логарифма правдоподобия модели $L_\bbf(\by,\DD,\bbf)$:
$$L_\bbf(\by,\DD,\bbf) = \log \PP(\by|\DD,\bbf) = \log \int_{\bw}\PP(\by|\DD,\bw)\PP(\bw|\bbf)d\bw$$
Введём априорное и апостериорные распределения вектора параметров:
$$\PP(\bw|\bbf) \sim N(\bmuo,\bAo), \ \PP(\bw|\by,\DD,\bbf) \sim N(\bmut,\bAt)$$

\end{frame}

\begin{frame}{Постановка задачи}
Рассмотрим вариационную нижнюю оценку $L_\bbf(\by,\DD,\bbf)$, полученную из неравенства Йенсена:
\begin{align*}
&L_\bbf(\by,\DD,\bbf)= \int_{\bw}\rho_2(\bw)\log\PP(\by|\DD,\bbf)d\bw \geq\\&\geq-\DKL(N(\bmut,\bAt)||N(\bmuo,\bAo))+\\
&+ \int_{\bw}\rho_2(\bw)\log\PP(\by|\DD,\bbf,\bw)d\bw = -L(\by,\DD,\bbf,\bw)
\end{align*}
Первое слагаемое назовём сложностью модели $L_\bw(\DD,\bbf)$:
$$L_\bw(\DD,\bbf) = \DKL(N(\bmut,\bAt)||N(\bmuo,\bAo)) $$
Второе слагаемое формулы является матожиданием правдоподобия выборки:
$$L_E(\by,\DD,\bbf,\bw) = \mathsf{E}_{\bw\sim N(\bmut,\bAt)}L_\DD(\by,\DD, \bbf,\bw) $$

\end{frame}

\begin{frame}{Постановка задачи}
	
Искомая модель $\bbf$ минимизирует суммарную функцию потерь
$$\bbf = \text{argmin}_{\bbf \in \FFF}L(\by,\DD,\bbf,\bw)$$
где 
$$L(\by,\DD,\bbf,\bw) = L_E(\by,\DD,\bbf,\bw) + L_\bw(\DD,\bbf,\bw),$$
$L_E(\by,\DD,\bbf,\bw)$ --- матожидание правдоподобия выборки, $L_\bw(\DD,\bbf,\bw)$ --- сложность модели.

\end{frame}

\begin{frame}{Предлагаемое решение}

Для оценки $L_E(\by,\DD,\bbf,\bw)$ воспользуемся интегрированием Монте-Карло:

$$L_E(\by,\DD,\bbf,\bw) \approx \frac1S\sum\limits_{k=1}^S L_\DD(\by,\DD, \bbf,\bw_k)$$

Сложность модели $L_\bw(\DD,\bbf,\bw)$ может быть найдена аналитически:

\begin{align*}
&L_\bw(\DD,\bbf,\bw) = \DKL(N(\bmuo,\bAo)||N(\bmut,\bAt)) = \frac12 \big( \log \frac{|\bAt|}{|\bAo|}-W+\\
&+\text{tr}(\textbf{A}_2\bAo)+(\bmuo-\bmut)^T\textbf{A}_2(\bmuo-\bmut) \big) 
\end{align*}

\end{frame}

\begin{frame}{Предлагаемое решение}

%\begin{enumerate}
Скалярные дисперсии и априорный вектор средних: $\bAo = \sigma\II , \ \bAt = \beta \II , \ \bmuo = \mu, \ \bmut = \bm$.

Находим $L_\bw(\DD,\bbf,\bw) = \sum\limits_{i=1}^W(\log\frac{\sigma}{\beta} + \frac{(\mu-m_i)^2 + \beta^2 + \sigma^2}{2\sigma^2})$

Необходимые условия экстремума:

$\frac\partial{\partial\mu}\DKL = \sum\limits_{i=1}^W\frac{\mu-m_i}{\sigma^2}=0 \Rightarrow \hat{\mu} = \frac1W\sum\limits_{i=1}^W m_i$.

$\frac\partial{\partial\sigma^2}\DKL = \sum\limits_{i=1}^W \frac1{2\sigma^2}-\frac{(\mu-m_i)^2 + \beta^2}{2\sigma^4}=0 \ \Rightarrow \ \hat{\sigma}^2 = \frac1W\sum\limits_{i=1}^W (\mu-m_i)^2 + \beta^2$.

%\end{enumerate}

\end{frame}

\begin{frame}{Предлагаемое решение}

Скалярные априорная дисперсия и вектор средних, апостериорная матрица диагональна: $\bAo = \sigma\II, \ \bAt = \text{diag}(\bs), \ \bmuo = \mu$, $\bmut = \bm$.

Тогда $L_\bw(\DD,\bbf,\bw) = \sum\limits_{i=1}^d(\log\frac{\sigma}{\sigma_i} + \frac{(\mu-m_i)^2 + \sigma_i^2 + \sigma^2}{2\sigma^2})$

$\frac\partial{\partial\mu}\DKL = \sum\limits_{i=1}^W\frac{\mu-m_i}{\sigma^2}=0 \ \Rightarrow \ \hat{\mu} = \frac1W\sum\limits_{i=1}^W m_i$.

$\frac\partial{\partial\sigma^2}\DKL = \sum\limits_{i=1}^W \frac1{2\sigma^2}-\frac{(\mu-m_i)^2 + \sigma_i^2}{2\sigma^4}=0 \ \Rightarrow \ \hat{\sigma}^2 = \frac1W\sum\limits_{i=1}^W (\mu-m_i)^2 + \sigma_i^2$.

\end{frame}

\begin{frame}{Алгоритм}

Оптимизация параметров сводится к следующему алгоритму:

\begin{enumerate}
	\item Инициализация $\bs = \textbf{0}, \ \bm = \textbf{0}, \ \mu = 0, \ \sigma^2 = 1$
	
	\item \textcolor{blue}{\textbf{Повторять}}:
	
	\item Сделать градиентный шаг $\bs:=\bs-\eta\nabla\bs, \ \bm:=\bm-\eta\nabla\bm$
	
	\item Обновить параметры априорного распределения $\mu:= \hat{\mu}, \ \sigma^2:=\hat{\sigma}^2$.
	
	\item \textcolor{blue}{\textbf{Пока}} значение $L$ не стабилизируется
	

\end{enumerate}

\end{frame}

%----------------------------------------------------------------------------------------------------------
\begin{frame}{Цели и данные эксперимента}

\begin{block}{Цели эксперимента}
Проверить работоспособность метода. Путём удаления наименее важных весов найти оптимальную структуру сети в задачах поиска парафраза.
\end{block}

\begin{block}{Данные}
Вычислительный эксперимент проводился на выборке SemEval~2015. Тренировочная, валидационная и тестовая выборки составили 70\%, 15\% и 15\% соответственно.
\end{block}

\end{frame}

\begin{frame}{Вычислительный эксперимент}

Для решения задачи использовалась рекуррентная нейронная сеть с одним скрытым слоем.
Векторизация слов проводилась методом GloVe.
%, основанного на факторизации матрицы слов-контекстов и использовании весовой функции для уменьшения значимости редких слов.

Вектор значений скрытого слоя обновляется как:
$$\mathbf{h_i} = \text{tanh}(\mathbf{x_i^TW} + \mathbf{h_{i-1}^TU} + \mathbf{b}),$$

где $\mathbf{x_i}\in R^m$ -- входной вектор, $\mathbf{h_i}\in R^n$, $\mathbf{W}\in R^{n*m}$, $\mathbf{U}\in R^{n*n}$, $\mathbf{b} \in R^n$.

\end{frame}


\begin{frame}{Вычислительный эксперимент}

В качестве метрики была выбрана F1-мера:

$$F_1 = 2 \frac{\text{precision} \cdot \text{recall}}{\text{precision} + \text{precision}}$$

%\vspace{-0.1cm}

\vspace{-0.5cm}

\begin{table}[H]
	\centering
	\caption{Результаты вычислительного эксперимента}
	\label{my-label1}
	\begin{tabular}{|l|l|}
		\hline
		Classificator          & F1-measure \\ \hline
		Logistic Regression    & 0.286      \\ \hline
		SVC                    & 0.290      \\ \hline
		DecisionTreeClassifier & 0.316      \\ \hline
		KNeighborsClassifier   & 0.322      \\ \hline
		RNN                    & 0.362      \\ \hline
		RNN+variational, I, I  & 0.311      \\ \hline
		RNN+variational, D, I  & 0.330      \\ \hline
	\end{tabular}
\end{table}

\end{frame}

%\begin{frame}{Результаты вычислительного эксперимента}
%
%%\begin{figure}
%%\includegraphics[height=2.5cm]{Pictures/Evidence_unprunned_I.png}
%%\caption{sad}
%%\end{figure}
%
%\begin{table}[h]
%	\centering
%	\begin{tabu}to \textwidth {X[c]X[c]}
%		\includegraphics[height=5.1cm]{Pictures/Evidence_unprunned_I.png}\captionof{figure}{Зависимость $L$ от числа эпох обучения} &\includegraphics[height=5.1cm]{Pictures/Train_test_score_unprunned_I.png}\captionof{figure}{Зависимость F1-меры на тренировочной и тестовой выборках от числа эпох} \\
%	\end{tabu}
%\end{table}
%
%\end{frame}

\begin{frame}{Результаты вычислительного эксперимента}

Чем больше плотность вероятности в нуле --- тем меньше важность параметра $\rho(0) \sim exp(-\frac{\mu_i^2}{2\sigma_i^2})$.

Обозначим $\lambda = \big|\frac{\mu_i}{\sigma_i}\big|$, получаем $\rho(0) \sim exp(-\frac{\lambda^2}{2})$.

%Для удаления весов используется параметр .% > \lambda
\smallskip
Веса с большим значением $\lambda$ имеют высокую плотность в нуле и могут быть удалены.

\end{frame}

\begin{frame}{Результаты вычислительного эксперимента}
Результаты удаления параметров для диагональной матрицы ковариаций представлены в таблице, где $p$ --- доля оставшихся параметров.
\vspace{-0.3cm}
\begin{table}[]
	\centering
	\label{my-label}
	\begin{tabular}{|l|l|l|l|l|l|}
		\hline
		\multirow{2}{*}{$\lambda$} & \multirow{2}{*}{$p$} & \multirow{2}{*}{Initial\_L} & \multirow{2}{*}{Retrain\_L} & \multirow{2}{*}{Initial\_score} & \multirow{2}{*}{Retrain\_score} \\
		&                                       &                                    &                                    &                                 &                                 \\ \hline
		0                 & 1.000                                 & 11431                              & 11431                              & 0.313                           & 0.313                           \\ \hline
		0.05              & 0.833                                 & 11306                              & 11219                              & 0.310                           & 0.336                           \\ \hline
		0.1               & 0.673                                 & 11198                              & 11102                              & 0.308                           & 0.327                           \\ \hline
		0.2               & 0.420                                 & 11071                              & 10851                              & 0.305                           & 0.331                           \\ \hline
		0.4               & 0.195                                 & 10841                              & 10710                              & 0.291                           & 0.321                           \\ \hline
		0.6               & 0.106                                 & 10827                              & 10545                              & 0.278                           & 0.311                           \\ \hline
		0.8               & 0.064                                 & 10925                              & 10502                              & 0.278                           & 0.311                           \\ \hline
		1.0                 & 0.040                                 & 10996                              & 10500                              & 0.278                           & 0.299                           \\ \hline
		1.2               & 0.025                                 & 11256                              & 10506                              & 0.277                           & 0.277                           \\ \hline
		1.4               & 0.018                                 & 11439                              & 10569                              & 0.276                           & 0.286                           \\ \hline
	\end{tabular}
\end{table}


%$\lambda$ & Доля\параметров & F1-мера до оптимизации & F1-мера после оптимизации & Bits/weight \\ \hline

\end{frame}

\begin{frame}{Результаты вычислительного эксперимента}
	
	
\begin{figure}
	\includegraphics[height=7cm]{Pictures/Portion_of_pruned_weights_on_lambda_D_I.pdf}
	\caption{Количество оставшихся параметров в зависимости от $\lambda$.}
\end{figure}
	
\end{frame}

\begin{frame}{Результаты вычислительного эксперимента}


\begin{figure}
\includegraphics[height=7cm]{Pictures/pruning_evidence_D_I.pdf}
\caption{Зависимость правдоподобия от $\lambda$.}
\end{figure}
%\includegraphics[height=4cm]{Pictures/Evidence_unprunned_I_I.pdf}
	
\end{frame}

\begin{frame}{Результаты вычислительного эксперимента}
	
\begin{figure}
	\includegraphics[height=7cm]{Pictures/lambda_D_I.pdf}
	\caption{Зависимость F1-меры от $\lambda$.}
\end{figure}
%\includegraphics[height=4cm]{Pictures/Evidence_unprunned_I_I.pdf}
	
\end{frame}


%----------------------------------------------------------------------------------------------------------
\begin{frame}{Заключение}

\begin{itemize}
\item Задача выбора оптимальной модели поставлена формально.
\item Минимизация правдоподобия модели не приводит к переобучению.
\item Алгоритм удаления параметров позволяет упростить структуру модели без существенных потерь качества.
\end{itemize}

\end{frame}
%----------------------------------------------------------------------------------------------------------
\end{document} 