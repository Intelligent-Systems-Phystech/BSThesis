\documentclass{beamer}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}
\usepackage{amsmath,mathrsfs,mathtext}
\usepackage{graphicx, epsfig}
\usepackage{amsmath}
\usepackage{lineno}
\usepackage{capt-of}
\usepackage{tabu}
\usepackage{multirow}
%\usepackage{graphicx}
%\usepackage{cite}
\usepackage{indentfirst}
\usepackage{pdfpages}
\usepackage{subfig}
\usepackage{caption}
%\usepackage{subcaption}
\usepackage{mwe} % For dummy images
\usetheme{Warsaw}%{Singapore}%{Warsaw}%{Warsaw}%{Darmstadt}
\usecolortheme{sidebartab}
\definecolor{beamer@blendedblue}{RGB}{0,99,111}
%----------------------------------------------------------------------------------------------------------

\newcommand{\PP}{p}
\newcommand{\DD}{{\mathfrak{D}}}
\newcommand{\FF}{{\mathcal{F}}}
%\newcommand{\AAA}{{\mathcal{A}}}
\newcommand{\FFF}{{\mathfrak{F}}}
\newcommand{\bw}{{\textbf{w}}}
\newcommand{\ba}{{\textbf{a}}}
\newcommand{\bb}{{\textbf{b}}}
\newcommand{\II}{{\textbf{I}}}
%\newcommand{\bmu}{{\boldsymbol{\mu}}}
\newcommand{\bbf}{{\textbf{f}}}
\newcommand{\by}{{\textbf{y}}}
\newcommand{\bm}{{\textbf{m}}}
\newcommand{\bs}{{\boldsymbol{\sigma}}}
\newcommand{\al}{{\alpha}}
\newcommand{\bal}{\boldsymbol{\alpha}}
\newcommand{\bbt}{\boldsymbol{\beta}}
%\newcommand{\bA}{\mathbf{A^\text{-1}}}
\newcommand{\bAo}{\mathbf{A^\text{-1}_\text{1}}}
\newcommand{\bAt}{\mathbf{A^\text{-1}_\text{2}}}
\newcommand{\bmuo}{{\boldsymbol{\mu}_1}}
\newcommand{\bmut}{{\boldsymbol{\mu}_2}}
\newcommand{\DKL}{\mathit{D}_{\text{KL}}}

\title[\hbox to 56mm{Выбор оптимальной модели рекуррентной сети\ \  \hfill\insertframenumber\,/\,\inserttotalframenumber}]
%{Выбор оптимальной модели рекуррентной сети \\ в задачах поиска парафраза}
{Выбор оптимальной модели рекуррентной сети в задачах поиска парафраза}
%\author[А.\,Н. Смердов]{\large \\Смердов Антон Николаевич}
\author[А.\,Н. Смердов]{{\large Смердов Антон Николаевич\\~} {\small \\ Научный руководитель \\ д.ф-м.н. В.В. Стрижов}}
\institute[МФТИ]{Московский физико-технический институт \\
	Факультет управления и прикладной математики\\
	Кафедра <<Интеллектуальные системы>>
}
\date{\footnotesize{МФТИ, 28 июня 2018}}

\bibliographystyle{unsrt}


%----------------------------------------------------------------------------------------------------------
\begin{document}
%----------------------------------------------------------------------------------------------------------
\begin{frame}
	%\thispagestyle{empty}
	\titlepage
\end{frame}
%-----------------------------------------------------------------------------------------------------

\begin{frame}{Решаемая задача и предлагаемый подход}

\begin{block}{Цель работы}
	%%Построение оптимальной модели рекуррентной нейросети в задачах поиска парафраза.
	%Построение модели глубокого обучения оптимальной структуры.
	%Построение и последующая оптимизация структуры нейросети.
Оптимизация структур моделей глубокого обучения и выбор модели с наибольшим правдоподобием.
%Оптимизация структур моделей глубокого обучения согласно 
%Выбор оптимальной модели 

\end{block}

\begin{block}{Проблема}
Избыточное число параметров в моделях глубокого обучения влечёт переобучение и сложность оптимизации параметров.
\end{block}

\begin{block}{Метод решения}
Используется вариационный байесовский подход с предположением о нормальном распределении вектора параметров модели. Для оптимизации структуры предлагается удалять параметры с наибольшей плотностью распределения в~нуле.

%Для оптимизации структуры используется прореживание полученной модели.

%Веса с наибольшей плотностью вероятности в нуле удаляются из сети.
%Наименее важные параметры удаляются из сети.
\end{block}

\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Литература}
%	\nocite{*}
%	\bibliography{papers_presentation2}
	
\begin{thebibliography}{}
	\bibitem{Sanborn}
	\textit{Sanborn A., Skryzalin J.} 
	Deep Learning for Semantic Similarity // CS224d: Deep Learning for Natural Language Processing~--- Stanford, CA, USA: Stanford University, 2015. Unpublished.
	
	\bibitem{Graves}
	%	\emph{Graves A.}
	\textit{Graves A.}
	Practical variational inference for neural networks // Advances in Neural Information Processing Systems 24 (NIPS 2011). P. 2348--2356.
	
	\bibitem{StrijovBakhteev}
	\textit{О.Ю. Бахтеев, В.В. Стрижов.}
	Выбор моделей глубокого обучения cубоптимальной сложности // Автоматика и телемеханика, 2018.
\end{thebibliography}{}
	
\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Задача нахождения оптимальной модели}
Дано $\DD = \{(\ba_i,\bb_i,y_i)\}, i = \overline{1,N}$,
$\ba_i, \bb_i$ --- последовательности векторов слов,
%$y_i \in \{0\dots5\}$
$y_i \in \mathbb{Y}$
--- экспертная оценка их близости. 

%Для модели $\bbf \in \FFF$ и вектора параметров $\bw$ определим логарифмическую функцию правдоподобия выборки:
%$$L_\DD(\by,\DD, \bbf,\bw) = \log\PP(\by|\DD,\bw,\bbf) = \sum_{(\ba,\bb,y)\in\DD} \log\PP(y|\ba,\bb,\bw,\bbf).$$
Оптимальная модель $\bbf$ находится максимизацией логарифма правдоподобия модели:% $L_\bbf(\by,\DD,\bbf)$:
$$L_\bbf(\by,\DD,\bbf) = \log \PP(\by|\DD,\bbf) = \log \int_{\bw}\PP(\by|\DD,\bw)\PP(\bw|\bbf)d\bw.$$

%Введём априорное и апостериорные распределения вектора параметров:
Априорное и апостериорное распределения параметров будем считать нормальными:
$$\PP(\bw|\bbf) \sim \mathcal{N}(\bmuo,\bAo), \ \PP(\bw|\by,\DD,\bbf) \sim \mathcal{N}(\bmut,\bAt).$$

Решение предлагается искать в классе $\mathfrak{F}$ рекуррентных нейронных сетей с одним скрытым слоем.

%, основанного на факторизации матрицы слов-контекстов и использовании весовой функции для уменьшения значимости редких слов.

Вектор значений скрытого слоя:
$$\mathbf{h}_i = \text{tanh}(\mathbf{x}_i^T\mathbf{W} + \mathbf{h}_{i-1}^T\mathbf{U} + \mathbf{b}),$$

где $\mathbf{x}_i\in R^m$ -- входной вектор, $\mathbf{h}_i\in R^n$, $\mathbf{W}\in R^{n \times m}$, $\mathbf{U}\in R^{n \times n}$, $\mathbf{b} \in R^n$.

\end{frame}

%\begin{frame}{Предлагаемая модель}
%%Для решения задачи использовалась рекуррентная нейронная сеть с одним скрытым слоем.
%Решение предлагается искать в классе $\mathfrak{F}$ рекуррентных нейронных сетей с одним скрытым слоем.
%
%%, основанного на факторизации матрицы слов-контекстов и использовании весовой функции для уменьшения значимости редких слов.
%
%Вектор значений скрытого слоя:
%$$\mathbf{h}_i = \text{tanh}(\mathbf{x}_i^T\mathbf{W} + \mathbf{h}_{i-1}^T\mathbf{U} + \mathbf{b}),$$
%
%где $\mathbf{x}_i\in R^m$ -- входной вектор, $\mathbf{h}_i\in R^n$, $\mathbf{W}\in R^{n \times m}$, $\mathbf{U}\in R^{n \times n}$, $\mathbf{b} \in R^n$.
%\end{frame}

\begin{frame}{Вариационная нижняя оценка $L_\bbf(\by,\DD,\bbf)$}
%Рассмотрим вариационную нижнюю оценку $L_\bbf(\by,\DD,\bbf)$, полученную 
Из неравенства Йенсена:
\begin{align*}
%&L_\bbf(\by,\DD,\bbf)= \int_{\bw}\rho_2(\bw)\log\PP(\by|\DD,\bbf)d\bw \geq\\&\geq-\DKL(N(\bmut,\bAt)||N(\bmuo,\bAo))+\\
%&+ \int_{\bw}\rho_2(\bw)\log\PP(\by|\DD,\bbf,\bw)d\bw = -L(\by,\DD,\bbf,\bw)
&L_\bbf(\by,\DD,\bbf)= \int_{\bw}\rho_2(\bw)\log\PP(\by|\DD,\bbf)d\bw \geq\\
&\geq-\underbrace{\DKL(\mathcal{N}(\bmut,\bAt)||\mathcal{N}(\bmuo,\bAo))}_{L_\bw(\DD,\bbf)}+ \underbrace{\int_{\bw}\rho_2(\bw)\log\PP(\by|\DD,\bbf,\bw)d\bw}_{-L_E(\by,\DD,\bbf,\bw)}.
\end{align*}
%Первое слагаемое назовём сложностью модели $L_\bw(\DD,\bbf)$:
%$$L_\bw(\DD,\bbf) = \DKL(N(\bmut,\bAt)||N(\bmuo,\bAo))$$
%$$L_\bw(\DD,\bbf) = \DKL(N(\bmut,\bAt)||N(\bmuo,\bAo))$$
Второе слагаемое является матожиданием правдоподобия выборки $L_\DD(\by,\DD, \bbf,\bw) = \sum_{(\ba,\bb,y)\in\DD} \log\PP(y|\ba,\bb,\bw,\bbf)$:
% Для модели $\bbf \in \FFF$ и вектора параметров $\bw$ определим логарифмическую функцию правдоподобия выборки:
$$L_E(\by,\DD,\bbf,\bw) = -\mathsf{E}_{\bw\sim \mathcal{N}(\bmut,\bAt)}L_\DD(\by,\DD, \bbf,\bw).$$
% Второе слагаемое формулы является матожиданием правдоподобия выборки:
% $$L_E(\by,\DD,\bbf,\bw) = \mathsf{E}_{\bw\sim N(\bmut,\bAt)}L_\DD(\by,\DD, \bbf,\bw) $$
Оптимизируемый функционал записывается в виде
$$L(\by,\DD,\bbf,\bw) = L_E(\by,\DD,\bbf,\bw) + L_\bw(\DD,\bbf,\bw),$$
оптимальная модель находится из выражения
%Искомая модель $\bbf$ минимизирует суммарную функцию потерь
$$\bbf = \text{argmin}_{\bbf \in \FFF}L(\by,\DD,\bbf,\bw).$$%, \text{где}$$


\end{frame}

%\begin{frame}{Постановка задачи}
%	
%Искомая модель $\bbf$ минимизирует суммарную функцию потерь
%$$\bbf = \text{argmin}_{\bbf \in \FFF}L(\by,\DD,\bbf,\bw)$$
%где 
%$$L(\by,\DD,\bbf,\bw) = L_E(\by,\DD,\bbf,\bw) + L_\bw(\DD,\bbf,\bw),$$
%$L_E(\by,\DD,\bbf,\bw)$ --- матожидание правдоподобия выборки, $L_\bw(\DD,\bbf,\bw)$ --- сложность модели.
%
%\end{frame}

\begin{frame}{Предлагаемое решение $L(\by,\DD,\bbf,\bw) \to \min$}

\begin{enumerate}
	\item 
Для оценки $L_E(\by,\DD,\bbf,\bw)$ воспользуемся интегрированием 
Монте-Карло:

$$L_E(\by,\DD,\bbf,\bw) \approx \frac1S\sum\limits_{k=1}^S L_\DD(\by,\DD, \bbf,\bw_k)$$

\item
Сложность модели $L_\bw(\DD,\bbf,\bw)$ может быть найдена аналитически:
\begin{align*}
&L_\bw(\DD,\bbf,\bw) = \DKL(\mathcal{N}(\bmuo,\bAo)||\mathcal{N}(\bmut,\bAt)) = \frac12 \big( \log \frac{|\bAt|}{|\bAo|}-\\
&-W+\text{tr}(\textbf{A}_2\bAo)+(\bmuo-\bmut)^T\textbf{A}_2(\bmuo-\bmut) \big) 
\end{align*}

\end{enumerate}

\end{frame}

%\begin{frame}{Предлагаемое решение}
%
%%\begin{enumerate}
%Скалярные дисперсии и априорный вектор средних: $\bAo = \sigma\II , \ \bAt = \beta \II , \ \bmuo = \mu, \ \bmut = \bm$.
%
%Находим $L_\bw(\DD,\bbf,\bw) = \sum\limits_{i=1}^W(\log\frac{\sigma}{\beta} + \frac{(\mu-m_i)^2 + \beta^2 + \sigma^2}{2\sigma^2})$
%
%Необходимые условия экстремума:
%
%$\frac\partial{\partial\mu}\DKL = \sum\limits_{i=1}^W\frac{\mu-m_i}{\sigma^2}=0 \Rightarrow \hat{\mu} = \frac1W\sum\limits_{i=1}^W m_i$.
%
%$\frac\partial{\partial\sigma^2}\DKL = \sum\limits_{i=1}^W \frac1{2\sigma^2}-\frac{(\mu-m_i)^2 + \beta^2}{2\sigma^4}=0 \ \Rightarrow \ \hat{\sigma}^2 = \frac1W\sum\limits_{i=1}^W (\mu-m_i)^2 + \beta^2$.
%
%%\end{enumerate}
%
%\end{frame}

\begin{frame}{Обновление параметров распределений}

%\begin{enumerate}
%\item
Скалярные априорная дисперсия и вектор средних, апостериорная матрица ковариаций \textbf{диагональна}: $\bAo = \sigma\II, \ \bAt = \text{diag}(\bs), \ \bmuo = \mu$, $\bmut = \bm$.

Тогда $L_\bw(\DD,\bbf,\bw) = \sum\limits_{i=1}^d(\log\frac{\sigma}{\sigma_i} + \frac{(\mu-m_i)^2 + \sigma_i^2 + \sigma^2}{2\sigma^2})$

$\frac\partial{\partial\mu}\DKL = \sum\limits_{i=1}^W\frac{\mu-m_i}{\sigma^2}=0 \ \Rightarrow \ \hat{\mu} = \frac1W\sum\limits_{i=1}^W m_i$.

$\frac\partial{\partial\sigma^2}\DKL = \sum\limits_{i=1}^W \frac1{2\sigma^2}-\frac{(\mu-m_i)^2 + \sigma_i^2}{2\sigma^4}=0 \ \Rightarrow \ \hat{\sigma}^2 = \frac1W\sum\limits_{i=1}^W (\mu-m_i)^2 + \sigma_i^2$.

%\item
\begin{block}{}
В частности, если апостериорная матрица ковариаций \textbf{скалярна}, т.е. $\bAt = \beta \II$:
%$\frac\partial{\partial\mu}\DKL = \sum\limits_{i=1}^W\frac{\mu-m_i}{\sigma^2}=0 \Rightarrow \hat{\mu} = \frac1W\sum\limits_{i=1}^W m_i$.
$\frac\partial{\partial\sigma^2}\DKL = \sum\limits_{i=1}^W \frac1{2\sigma^2}-\frac{(\mu-m_i)^2 + \beta^2}{2\sigma^4}=0 \ \Rightarrow \ \hat{\sigma}^2 = \frac1W\sum\limits_{i=1}^W (\mu-m_i)^2 + \beta^2$.
\end{block}

%\end{enumerate}

\end{frame}

\begin{frame}{Алгоритм обновления параметров распределений}

Оптимизация параметров:

\begin{enumerate}
	\item Инициализация $\bm = \textbf{0}, \ \bs = \textbf{1}, \ \mu = 0, \ \sigma^2 = 1$
	
	\item \textcolor{blue}{\textbf{Повторять}}:
	
	\item Сделать градиентный шаг $\bs:=\bs-\eta\nabla\bs, \ \bm:=\bm-\eta\nabla\bm$
	
	\item Обновить параметры априорного распределения $\mu:= \hat{\mu}, \ \sigma^2:=\hat{\sigma}^2$.
	
	\item \textcolor{blue}{\textbf{Пока}} значение $L$ не стабилизируется
	

\end{enumerate}

\end{frame}

%----------------------------------------------------------------------------------------------------------
\begin{frame}{Проверка метода для задачи поиска парафраза}

\begin{block}{Цели вычислительного эксперимента}
Проверить работоспособность метода. Путём удаления наименее важных весов найти оптимальную структуру сети в задачах поиска парафраза.
\end{block}

\begin{block}{Данные}
Вычислительный эксперимент проводился на выборке пар предложений разной степени схожести SemEval~2015. Тренировочная, валидационная и тестовая выборки составили 70\%, 15\% и 15\% соответственно.
\end{block}

Векторизация слов для использования алгоритмами проводилась методом GloVe.

Функционалом качества была выбрана F1-мера:
$$F_1 = \frac{2 \cdot \text{precision} \cdot \text{recall}}{\text{precision} + \text{precision}}$$

\end{frame}



%\begin{frame}{Вычислительный эксперимент}
%
%Для решения задачи использовалась рекуррентная нейронная сеть с одним скрытым слоем.
%Векторизация слов проводилась методом GloVe.
%%, основанного на факторизации матрицы слов-контекстов и использовании весовой функции для уменьшения значимости редких слов.
%
%Вектор значений скрытого слоя обновляется как:
%$$\mathbf{h_i} = \text{tanh}(\mathbf{x_i^TW} + \mathbf{h_{i-1}^TU} + \mathbf{b}),$$
%
%где $\mathbf{x_i}\in R^m$ -- входной вектор, $\mathbf{h_i}\in R^n$, $\mathbf{W}\in R^{n*m}$, $\mathbf{U}\in R^{n*n}$, $\mathbf{b} \in R^n$.
%
%\end{frame}

\begin{frame}{Сравнение с базовыми алгоритмами}


%\vspace{-0.1cm}

\vspace{-0.5cm}

\begin{table}[H]
	\centering
	\caption*{Результаты вычислительного эксперимента:}
	\label{my-label1}
	\begin{tabular}{|l|l|}
		\hline
		Classificator          & F1-measure \\ \hline
		Logistic Regression    & 0.286      \\ \hline
		SVC                    & 0.290      \\ \hline
		DecisionTreeClassifier & 0.316      \\ \hline
		KNeighborsClassifier   & 0.322      \\ \hline
		RNN                    & 0.362      \\ \hline
		RNN+variational, I, I  & 0.311      \\ \hline
		RNN+variational, D, I  & 0.330      \\ \hline
	\end{tabular}
\end{table}

\end{frame}

%\begin{frame}{Результаты вычислительного эксперимента}
%
%%\begin{figure}
%%\includegraphics[height=2.5cm]{Pictures/Evidence_unprunned_I.png}
%%\caption{sad}
%%\end{figure}
%
%\begin{table}[h]
%	\centering
%	\begin{tabu}to \textwidth {X[c]X[c]}
%		\includegraphics[height=5.1cm]{Pictures/Evidence_unprunned_I.png}\captionof{figure}{Зависимость $L$ от числа эпох обучения} &\includegraphics[height=5.1cm]{Pictures/Train_test_score_unprunned_I.png}\captionof{figure}{Зависимость F1-меры на тренировочной и тестовой выборках от числа эпох} \\
%	\end{tabu}
%\end{table}
%
%\end{frame}

%\begin{frame}{Результаты вычислительного эксперимента}
%
%Чем больше плотность вероятности в нуле --- тем меньше важность параметра $\rho(0) \sim exp(-\frac{\mu_i^2}{2\sigma_i^2})$.
%
%Обозначим $\lambda = \big|\frac{\mu_i}{\sigma_i}\big|$, получаем $\rho(0) \sim exp(-\frac{\lambda^2}{2})$.
%
%%Для удаления весов используется параметр .% > \lambda
%\smallskip
%Веса с большим значением $\lambda$ имеют высокую плотность в нуле и могут быть удалены.
%
%\end{frame}

%\begin{frame}{Результаты вычислительного эксперимента}
%Результаты удаления параметров для диагональной матрицы ковариаций представлены в таблице, где $p$ --- доля оставшихся параметров.
%\vspace{-0.3cm}
%\begin{table}[]
%	\centering
%	\label{my-label}
%	\begin{tabular}{|l|l|l|l|l|l|}
%		\hline
%		\multirow{2}{*}{$\lambda$} & \multirow{2}{*}{$p$} & \multirow{2}{*}{Initial\_L} & \multirow{2}{*}{Retrain\_L} & \multirow{2}{*}{Initial\_score} & \multirow{2}{*}{Retrain\_score} \\
%		&                                       &                                    &                                    &                                 &                                 \\ \hline
%		0                 & 1.000                                 & 11431                              & 11431                              & 0.313                           & 0.313                           \\ \hline
%		0.05              & 0.833                                 & 11306                              & 11219                              & 0.310                           & 0.336                           \\ \hline
%		0.1               & 0.673                                 & 11198                              & 11102                              & 0.308                           & 0.327                           \\ \hline
%		0.2               & 0.420                                 & 11071                              & 10851                              & 0.305                           & 0.331                           \\ \hline
%		0.4               & 0.195                                 & 10841                              & 10710                              & 0.291                           & 0.321                           \\ \hline
%		0.6               & 0.106                                 & 10827                              & 10545                              & 0.278                           & 0.311                           \\ \hline
%		0.8               & 0.064                                 & 10925                              & 10502                              & 0.278                           & 0.311                           \\ \hline
%		1.0                 & 0.040                                 & 10996                              & 10500                              & 0.278                           & 0.299                           \\ \hline
%		1.2               & 0.025                                 & 11256                              & 10506                              & 0.277                           & 0.277                           \\ \hline
%		1.4               & 0.018                                 & 11439                              & 10569                              & 0.276                           & 0.286                           \\ \hline
%	\end{tabular}
%\end{table}


%$\lambda$ & Доля\параметров & F1-мера до оптимизации & F1-мера после оптимизации & Bits/weight \\ \hline

%\end{frame}

\begin{frame}{Результаты вычислительного эксперимента}
	
\begin{columns}
	\column{6.2cm}
	
%	\vspace{-0.2cm}	
	
	Чем больше плотность вероятности в нуле $\rho(0)=\frac{1}{\sqrt{2\pi\sigma_i^2}}exp(-\frac{\mu_i^2}{2\sigma_i^2})$, тем меньше важность параметра.

	Обозначим отношение сигнал-шум за $\lambda = \big|\frac{\mu_i}{\sigma_i}\big|$, тогда $\rho(0) \sim exp(-\frac{\lambda^2}{2})$.
	%Для удаления весов используется параметр .% > \lambda
%	\smallskip
	Параметры с большим значением $\lambda$ могут быть удалены.
%	Веса с большим значением $\lambda$ имеют высокую плотность в нуле и могут быть удалены.	
	
	\vspace{0.1cm}
	
	\begin{figure}
	\includegraphics[width=5.4cm]{Pictures/lambda_D_I.pdf}
	\vspace{-0.45cm}
	\caption*{Зависимость F1-меры от $\lambda$.}
	\end{figure}
	
%	\column{0.6\textwidth}

%	\hspace{-2cm}
	
	\column{5.6cm}

	\vspace{-0.55cm}
	\begin{figure}
%	\hspace{-2cm}
	\includegraphics[width=5.4cm]{Pictures/Portion_of_pruned_weights_on_lambda_D_I.pdf}
	\vspace{-0.45cm}	
%	\hspace{-2cm}
	\caption*{Количество параметров от $\lambda$.}
	\end{figure}
	
	\vspace{-1.2cm}
	
	\begin{figure}
	\includegraphics[width=5.4cm]{Pictures/pruning_evidence_D_I.pdf}
	\vspace{-0.45cm}	
	\caption*{Зависимость правдоподобия от $\lambda$.}
	\end{figure}
	
\end{columns}
%
%	
%	
%\begin{figure}
%	\includegraphics[height=7cm]{Pictures/Portion_of_pruned_weights_on_lambda_D_I.pdf}
%	\caption*{Количество оставшихся параметров в зависимости от $\lambda$.}
%\end{figure}
\end{frame}

%\begin{frame}{Результаты вычислительного эксперимента}
%
%
%\begin{figure}
%\includegraphics[height=7cm]{Pictures/pruning_evidence_D_I.pdf}
%\caption*{Зависимость правдоподобия от $\lambda$.}
%\end{figure}
%%\includegraphics[height=4cm]{Pictures/Evidence_unprunned_I_I.pdf}
%	
%\end{frame}
%
%\begin{frame}{Результаты вычислительного эксперимента}
%	
%\begin{figure}
%	\includegraphics[height=7cm]{Pictures/lambda_D_I.pdf}
%	\caption*{Зависимость F1-меры от $\lambda$.}
%\end{figure}
%%\includegraphics[height=4cm]{Pictures/Evidence_unprunned_I_I.pdf}
%	
%\end{frame}

%\begin{frame}
%	{\centering
%		\begin{table}[ht]
%			\begin{tabular}{cc}
%%				\begin{subfigure}{0.4\textwidth}\centering\includegraphics[width=0.3\columnwidth]{example-image-a}\caption{Figure A}\label{fig:taba}\end{subfigure}&
%%				\begin{subfigure}{0.4\textwidth}\centering\includegraphics[width=0.3\columnwidth]{example-image-b}\caption{Figure B}\label{fig:tabb}\end{subfigure}\\
%%				\newline
%%				\begin{subfigure}{0.4\textwidth}\centering\includegraphics[width=0.3\columnwidth]{example-image-c}\caption{Figure C}\label{fig:tabc}\end{subfigure}&
%%				\begin{subfigure}{0.4\textwidth}\centering\includegraphics[width=0.3\columnwidth]{example-image-a}\caption{Figure A again}\label{fig:taba2}\end{subfigure}\\
%%			\end{tabular}
%			\caption{A table with figures}
%			\label{tab:mytable}
%		\end{table}
%	}
%	
%	We can see that Figure \ref{fig:taba} and Figure \ref{fig:taba2} is the same. Also the table counter value is used for the reference.
%\end{frame}

%----------------------------------------------------------------------------------------------------------
\begin{frame}{Заключение}

\begin{itemize}
\item Предложена реализация байесовского вывода для задачи поиска парафраза.
%\item Задача выбора оптимальной модели поставлена формально.
\item Минимизация правдоподобия модели не приводит к переобучению.
\item Алгоритм удаления параметров позволяет упростить структуру модели без существенных потерь качества.
\end{itemize}

\begin{block}{Публикации}
\textit{А.\,Н.\,Смердов, О.\,Ю.\,Бахтеев, В.\,В\,Стрижов}. Выбор оптимальной модели рекуррентной сети в задачах поиска парафраза // Информатика и её применения, 2019. Том 13, выпуск 2.
\end{block}


\end{frame}
%----------------------------------------------------------------------------------------------------------
\end{document} 